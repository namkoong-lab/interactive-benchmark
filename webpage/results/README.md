# Results Directory

This directory contains evaluation results organized by model. Each model has its own subdirectory with JSON files for each benchmark type it participated in.

## Directory Structure

The results directory is organized by model:

```
results/
├── model-name-1/
│   ├── variable_category.json    # Optional: Results for Variable Category benchmark
│   ├── variable_persona.json     # Optional: Results for Variable Persona benchmark
│   ├── variable_settings.json    # Optional: Results for Variable Settings benchmark
│   ├── metadata.json             # Optional: Model metadata
│   └── README.md                 # Optional: Additional information
├── model-name-2/
│   ├── variable_category.json
│   └── variable_persona.json
└── ...
```

## Submission Format

Each model folder should contain JSON files for the benchmarks it participated in:

- `variable_category.json` - Results for the Variable Category benchmark (fixed persona, varying categories)
- `variable_persona.json` - Results for the Variable Persona benchmark (fixed category, varying personas)
- `variable_settings.json` - Results for the Variable Settings benchmark (both vary)

You can submit to one, two, or all three leaderboards by including the corresponding JSON files.

## Results JSON Format

Each JSON file should follow the format generated by the benchmark experiment runner:

```json
{
  "experiment_type": "variable_category",
  "model": "your-model-name",
  "context_mode": "raw",
  "feedback_type": "persona",
  "prompting_tricks": "none",
  "config_file_path": "configs/benchmark_configs/variable_category.yaml",
  "regret_progression": {
    "all_seed_data": [[...], [...]],
    "mean": [...],
    "standard_error": [...]
  },
  "questions_progression": {
    "all_seed_data": [[...], [...]],
    "mean": [...],
    "standard_error": [...]
  }
}
```

The JSON file should include:
- Configuration metadata (experiment_type, model, context_mode, feedback_type, etc.)
- Performance metrics (regret_progression, questions_progression)
- Config file path reference for reproducibility

## How to Submit

1. Create a folder with your model name (if it doesn't exist)
2. For each benchmark you want to submit:
   - Run the corresponding benchmark experiment
   - Copy your `results.json` file to your model folder
   - Rename it to `{benchmark-type}.json` (e.g., `variable_category.json`)
3. Submit a pull request

**Example:**
- Model: `gpt-4o`
- Folder: `results/gpt-4o/`
- Files: 
  - `variable_category.json`
  - `variable_persona.json`
  - `variable_settings.json` (optional)

## Automatic Leaderboard Updates

The leaderboard will automatically read JSON files from model folders and display them on the website. When you submit a PR with your results, the leaderboard will update automatically once merged.

For detailed submission instructions, see the [Submit page](../../src/pages/Submit.tsx).

